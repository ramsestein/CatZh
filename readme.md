# ğŸŒ Benchmark de TraducciÃ³n CatalÃ¡n-Chino mediante EvaluaciÃ³n LLM

## ğŸ“Š Resumen del Proyecto

Sistema innovador de evaluaciÃ³n de calidad de traducciÃ³n automÃ¡tica catalÃ¡nâ†’chino que utiliza la capacidad de LLMs para responder preguntas SÃ­/No como mÃ©trica de calidad. Este benchmark evalÃºa 6 modelos de traducciÃ³n mediante 3 LLMs evaluadores sobre un dataset de 612 preguntas en 6 estilos lingÃ¼Ã­sticos diferentes.

### ğŸ¯ CaracterÃ­sticas Principales

- **612 preguntas SÃ­/No** en catalÃ¡n con respuestas esperadas en chino (æ˜¯/ä¸æ˜¯)
- **6 modelos de traducciÃ³n** evaluados: NLLB-200, AINA, Google Translate, M2M100, mBART, OpusMT
- **3 LLMs evaluadores**: Qwen3:0.6b, Yi:9b, DeepSeek-R1:1.5b
- **6 estilos lingÃ¼Ã­sticos**: Desde formal tÃ©cnico hasta minimalista extremo
- **Sistema de ensemble** con ranking semÃ¡ntico usando 3 modelos de embeddings
- **9,040+ evaluaciones** por modelo (5 iteraciones Ã— 612 preguntas Ã— 3 LLMs)

## ğŸ”¬ MetodologÃ­a

### Pipeline de EvaluaciÃ³n de 3 Fases

**FASE 1: TRADUCCIÃ“N**
- Dataset Original: 612 preguntas en catalÃ¡n
- 6 Modelos de TraducciÃ³n: NLLB-200, AINA, Google Translate, M2M100, mBART, OpusMT
- Resultado: 6 datasets traducidos al chino

**FASE 2: EVALUACIÃ“N CON LLMs**
- 3 LLMs evalÃºan cada dataset traducido
- 5 iteraciones por pregunta para robustez estadÃ­stica
- MÃ©tricas: exact_match, contains_correct, had_think_tags
- Total: 9,040 evaluaciones por modelo

**FASE 3: ENSEMBLE SEMÃNTICO**
- Ranking usando 3 modelos de embeddings: LaBSE, BGE-large-zh, Paraphrase-multilingual-mpnet
- VotaciÃ³n por consenso para determinar mejores traducciones
- GeneraciÃ³n de datasets mejorados (top-2 y top-3)

### MÃ©tricas de EvaluaciÃ³n

- **exact_match**: Coincidencia exacta con respuesta esperada (æ˜¯/ä¸æ˜¯) [0-100%]
- **contains_correct**: Respuesta contiene el elemento correcto [0-100%]
- **had_think_tags**: Modelo usÃ³ tags de razonamiento interno [0-100%]
- **semantic_similarity**: Similitud semÃ¡ntica entre traducciones [0-1]

## ğŸ“ˆ Resultados Principales

### Ranking Global de Modelos de TraducciÃ³n

| Ranking | Modelo | Exact Match | Contains Correct |
|---------|--------|-------------|------------------|
| 1Âº | **AINA** | 46.98% | 68.97% |
| 2Âº | **Google Translate** | 41.97% | 63.28% |
| 3Âº | **Ensemble (Top-3)** | 25.84% | 62.35% |
| 4Âº | **mBART** | 19.55% | 54.28% |
| 5Âº | **M2M100** | 16.60% | 53.88% |
| 6Âº | **CAT (Directo)Â¹** | 2.70% | 89.85% |
| 7Âº | **Llama3.2:3bÂ²** | 0.00% | 97.32% |

Â¹ *CAT: EvaluaciÃ³n directa en catalÃ¡n sin traducciÃ³n - Alta comprensiÃ³n semÃ¡ntica (89.85%) pero falla en formato de respuesta chino*  
Â² *Llama3.2:3b: Modelo base de trabajo del proyecto*

### Rendimiento por Estilo LingÃ¼Ã­stico (Exact Match % - Promedio entre evaluadores)

| Modelo | Formal TÃ©cnico | Errores Gramaticales | Minimalista Extremo | Normal TelegrÃ¡fico | Slang Juvenil | Verboso PoÃ©tico |
|--------|----------------|---------------------|-------------------|------------------|---------------|-----------------|
| **AINA** | 56.5% | 45.1% | 28.1% | 53.0% | 50.9% | 41.1% |
| **Google** | 50.1% | 46.1% | 18.3% | 37.6% | 43.6% | 34.4% |
| **Ensemble** | 31.0% | 21.2% | 16.1% | 31.6% | 24.8% | 30.4% |
| **mBART** | 27.7% | 20.7% | 13.7% | 18.4% | 20.2% | 20.2% |
| **M2M100** | 23.8% | 7.3% | 19.7% | 15.5% | 12.2% | 21.2% |
| **CAT** | 3.3% | 0.3% | 1.1% | 4.6% | 2.3% | 4.2% |
| **Llama3.2** | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |

### MÃ©tricas Detalladas por Modelo Evaluador

#### Modelo: AINA
| LLM Evaluador | Exact Match | Contains Correct |
|---------------|-------------|------------------|
| Qwen3:0.6b | 55.64% | 70.86% |
| Yi:9b | 36.10% | 72.60% |
| DeepSeek-R1:1.5b | 49.06% | 63.47% |

#### Modelo: Google Translate  
| LLM Evaluador | Exact Match | Contains Correct |
|---------------|-------------|------------------|
| Qwen3:0.6b | 57.98% | 65.05% |
| Yi:9b | 26.32% | 66.71% |
| DeepSeek-R1:1.5b | 41.57% | 58.03% |

#### Modelo: CAT (EvaluaciÃ³n Directa en CatalÃ¡n)
| LLM Evaluador | Exact Match | Contains Correct |
|---------------|-------------|------------------|
| Qwen3:0.6b | 7.85% | 88.25% |
| Yi:9b | 0.00% | 95.01% |
| DeepSeek-R1:1.5b | 0.03% | 86.59% |

### Hallazgos Clave

1. **AINA lidera en precisiÃ³n global** con el mejor balance entre exact match (46.98%) y comprensiÃ³n semÃ¡ntica (68.97%), estableciÃ©ndose como el modelo de traducciÃ³n mÃ¡s efectivo para el par catalÃ¡nâ†’chino

2. **Experimento de control (CAT)**: La evaluaciÃ³n directa en catalÃ¡n demuestra que los LLMs comprenden excelentemente el contenido (89.85% contains_correct) pero no pueden generar respuestas en el formato chino esperado (æ˜¯/ä¸æ˜¯), validando la necesidad de traducciÃ³n

3. **Modelo base Llama3.2:3b**: Aunque muestra comprensiÃ³n casi perfecta del contenido (97.32%), su incapacidad para generar el formato de respuesta correcto lo hace inadecuado como traductor directo, justificando el pipeline de traducciÃ³n+evaluaciÃ³n

4. **Impacto del estilo lingÃ¼Ã­stico**: 
   - **Formal tÃ©cnico**: Mejores resultados generales (AINA: 56.5%, Google: 50.1%)
   - **Minimalista extremo**: El mÃ¡s desafiante para todos los modelos (AINA: 28.1%, Google: 18.3%)
   - **Errores gramaticales**: Impacto moderado en la precisiÃ³n, sugiriendo robustez de los modelos

5. **Efecto del razonamiento interno**: 
   - Modelos con think tags (Qwen3, DeepSeek-R1): Mayor precisiÃ³n en exact match
   - Yi:9b sin think tags: Mayor contains_correct pero menor precisiÃ³n exacta
   - CorrelaciÃ³n positiva entre uso de razonamiento interno y precisiÃ³n de formato

6. **Ensemble semÃ¡ntico**: Con 25.84% de exact match, el ensemble no supera a los mejores modelos individuales pero ofrece valor para aplicaciones que requieren diversidad y consenso entre traducciones

### DistribuciÃ³n de Evaluaciones

- **Total de evaluaciones**: 54,172 (promedio 9,029 por modelo)
- **Preguntas evaluadas**: 612 en 6 estilos lingÃ¼Ã­sticos distintos
- **Iteraciones por pregunta**: 5 para robustez estadÃ­stica
- **LLMs evaluadores**: 3 modelos (Qwen3:0.6b, Yi:9b, DeepSeek-R1:1.5b)
- **Modelos de traducciÃ³n evaluados**: 6 + 2 controles (CAT directo, Llama3.2:3b base)

## ğŸ“‚ Dataset de EvaluaciÃ³n

El dataset consta de 612 preguntas organizadas en 6 bloques estilÃ­sticos:

| Bloque | Rango | Estilo | CaracterÃ­sticas |
|--------|-------|--------|-----------------|
| 1 | 1-102 | Normal telegrÃ¡fico | Formato estÃ¡ndar, conciso |
| 2 | 103-204 | Formal tÃ©cnico | Lenguaje culto, ligeramente verborreico |
| 3 | 205-306 | Slang juvenil | Jerga callejera, informal |
| 4 | 307-408 | Con errores | Errores gramaticales no agresivos |
| 5 | 409-510 | Verboso poÃ©tico | Arcaico, muy elaborado |
| 6 | 511-612 | Minimalista extremo | MÃ­nima expresiÃ³n posible |

## ğŸš€ Uso del Sistema

### InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/catalan-chinese-benchmark.git
cd catalan-chinese-benchmark

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Instalar Ollama (para evaluaciÃ³n con LLMs locales)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen3:0.6b
ollama pull yi:9b
ollama pull deepseek-r1:1.5b
```

### Ejecutar EvaluaciÃ³n Completa

```
bash
# FASE 1: Traducir dataset con todos los modelos
python t2_nllb200.py datasets/test-catalan.jsonl --output_file test-llm-nllb200.jsonl
python t2_aina.py datasets/test-catalan.jsonl --output_file test-llm-aina.jsonl
python t2_google.py datasets/test-catalan.jsonl --output_file test-llm-gt.jsonl
python t2_m2m100.py datasets/test-catalan.jsonl --output_file test-llm-m2m100.jsonl
python t2_mbart.py datasets/test-catalan.jsonl --output_file test-llm-mbart.jsonl
python t2_opus.py datasets/test-catalan.jsonl --output_file test-llm-opus.jsonl
```

## FASE 2: Evaluar con Ollama
```
bash
python t1_ollama.py
# ConfiguraciÃ³n interactiva:
# - Workers paralelos: 10 (ajustar segÃºn CPU)
# - Modo TEST: No (para evaluaciÃ³n completa)
```

## FASE 3: Crear ensemble semÃ¡ntico
```
bash
python t3_ensemble.py --data_dir . --output_dir ./ranked_datasets --top_k 2 3
```
### EvaluaciÃ³n RÃ¡pida (Modo Test)

```
bash
# Solo 10 preguntas para prueba rÃ¡pida
python t1_ollama.py --test_mode --num_test 10
```

### ğŸ“Š Estructura del Proyecto

```
catalan-chinese-benchmark/
â”œâ”€â”€ README.md                          # Este archivo
â”œâ”€â”€ requirements.txt                   # Dependencias Python
â”œâ”€â”€ requirements_tpu.txt              # Dependencias para TPU/Colab
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ test-catalan.jsonl           # Dataset original (612 preguntas)
â”‚   â””â”€â”€ explain_dataset.txt          # DescripciÃ³n de bloques
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ t1_ollama.py                 # EvaluaciÃ³n con LLMs locales
â”‚   â”œâ”€â”€ t2_nllb200.py                # TraducciÃ³n con NLLB-200
â”‚   â”œâ”€â”€ t2_aina.py                   # TraducciÃ³n con AINA
â”‚   â”œâ”€â”€ t2_google.py                 # TraducciÃ³n con Google
â”‚   â”œâ”€â”€ t2_m2m100.py                 # TraducciÃ³n con M2M100
â”‚   â”œâ”€â”€ t2_mbart.py                  # TraducciÃ³n con mBART
â”‚   â”œâ”€â”€ t2_opus.py                   # TraducciÃ³n con OpusMT
â”‚   â””â”€â”€ t3_ensemble.py               # Ensemble semÃ¡ntico
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ summary_results_*.json       # Resultados por modelo
â”‚   â”œâ”€â”€ ranking_statistics.json      # EstadÃ­sticas de ensemble
â”‚   â””â”€â”€ results_direct/              # Resultados detallados
â”‚
â”œâ”€â”€ training/                         # Sistema de entrenamiento (opcional)
â”‚   â”œâ”€â”€ train_all.py                 # Entrenamiento multi-modelo
â”‚   â”œâ”€â”€ universal_trainer.py         # LÃ³gica de entrenamiento
â”‚   â”œâ”€â”€ model_configs.py             # Configuraciones de modelos
â”‚   â””â”€â”€ checkpoint_manager.py        # GestiÃ³n de checkpoints
â”‚
â””â”€â”€ ranked_datasets/                  # Datasets generados por ensemble
    â”œâ”€â”€ top2_ranked_dataset.jsonl    # Mejores 2 traducciones
    â””â”€â”€ top3_ranked_dataset.jsonl    # Mejores 3 traducciones
```


## ğŸ“‰AnÃ¡lisis de Resultados

### Hallazgos Clave

- OpusMT: Paradoja interesante - 97.3% contains_correct pero 0% exact_match, sugiere que entiende el contenido pero no respeta el formato de respuesta
- NLLB y AINA: Mejor balance entre comprensiÃ³n y formato, lideran en el ranking del ensemble
- Google Translate: Rendimiento sÃ³lido y consistente en todos los estilos lingÃ¼Ã­sticos

Impacto del estilo: El bloque minimalista extremo es el mÃ¡s desafiante para todos los modelos
Think tags: Los modelos que usan razonamiento interno (Qwen3, DeepSeek) muestran mayor precisiÃ³n

### Contribuciones del Ensemble (Top-2)

AINA: 340 contribuciones (27.8%)
NLLB: 321 contribuciones (26.2%)
M2M100: 257 contribuciones (21.0%)
Google: 195 contribuciones (15.9%)
mBART: 111 contribuciones (9.1%)

### ğŸ”§ConfiguraciÃ³n Avanzada
Ajustar ParÃ¡metros de EvaluaciÃ³n
```
`python
# En t1_ollama.py
config.iterations_per_question = 10  # MÃ¡s iteraciones para mayor robustez
config.max_parallel_calls = 20       # MÃ¡s workers para evaluaciÃ³n rÃ¡pida
config.timeout_seconds = 120         # Timeout mayor para modelos lentos
```

### Agregar Nuevo Modelo de TraducciÃ³n

Crear script t2_nuevo_modelo.py basado en plantilla existente
Implementar clase NuevoModeloTranslator
Agregar a pipeline de evaluaciÃ³n

### Agregar Nuevo LLM Evaluador
```
python
# En t1_ollama.py, agregar a config.llm_models
self.llm_models = [
    "qwen3:0.6b",
    "yi:9b",
    "deepseek-r1:1.5b",
    "nuevo-modelo:version"  # Nuevo LLM
]
```

## ğŸ“Reproducibilidad

Para reproducir exactamente los resultados:

Versiones de software: Ver requirements.txt para versiones exactas
Semillas aleatorias: Todas fijadas a 42
Datasets: Disponibles en carpeta datasets/
ConfiguraciÃ³n: Documentada en archivos de configuraciÃ³n

## ğŸ¤Contribuciones

Las contribuciones son bienvenidas. Por favor:

Fork el repositorio
Crea una rama para tu feature (git checkout -b feature/NuevaCaracteristica)
Commit tus cambios (git commit -m 'Agregar nueva caracterÃ­stica')
Push a la rama (git push origin feature/NuevaCaracteristica)
Abre un Pull Request

## ğŸ“„Licencia
Este proyecto estÃ¡ bajo licencia MIT. Ver archivo LICENSE para mÃ¡s detalles.

## ğŸ™Agradecimientos

Modelos de traducciÃ³n de cÃ³digo abierto
Comunidad Ollama por infraestructura de LLMs locales
Sentence Transformers por modelos de embeddings

