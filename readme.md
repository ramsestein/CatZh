# üåê Introducci√≥n de modelos de traducci√≥n para mejora de rendimiento de LLM

# Tareas pte
- Traductor en tandem aina + qwen3 -> Flores200
- Corroborar lo de semantica correlaci√≥n con modelo grande

## üìä Resumen del Proyecto

Sistema innovador de evaluaci√≥n de calidad de traducci√≥n autom√°tica catal√°n‚Üíchino que utiliza la capacidad de LLMs para responder preguntas S√≠/No como m√©trica de calidad. Este benchmark eval√∫a 6 modelos de traducci√≥n mediante 3 LLMs evaluadores sobre un dataset de 612 preguntas en 6 estilos ling√º√≠sticos diferentes.

### üéØ Caracter√≠sticas Principales

- **612 preguntas S√≠/No** en catal√°n con respuestas esperadas en chino (ÊòØ/‰∏çÊòØ)
- **6 modelos de traducci√≥n** evaluados: NLLB-200, AINA, Google Translate, M2M100, mBART, OpusMT
- **3 LLMs evaluadores**: Qwen3:0.6b, Yi:9b, DeepSeek-R1:1.5b
- **6 estilos ling√º√≠sticos**: Desde formal t√©cnico hasta minimalista extremo
- **Sistema de ensemble** con ranking sem√°ntico usando 3 modelos de embeddings
- **9,040+ evaluaciones** por modelo (5 iteraciones √ó 612 preguntas √ó 3 LLMs)

## üî¨ Metodolog√≠a

### Pipeline de Evaluaci√≥n de 3 Fases

**FASE 1: TRADUCCI√ìN**
- Dataset Original: 612 preguntas en catal√°n
- 6 Modelos de Traducci√≥n: NLLB-200, AINA, Google Translate, M2M100, mBART, OpusMT
- Resultado: 6 datasets traducidos al chino

**FASE 2: EVALUACI√ìN CON LLMs**
- 3 LLMs eval√∫an cada dataset traducido
- 5 iteraciones por pregunta para robustez estad√≠stica
- M√©tricas: exact_match, contains_correct, had_think_tags
- Total: 9,040 evaluaciones por modelo

**FASE 3: ENSEMBLE SEM√ÅNTICO**
- Ranking usando 3 modelos de embeddings: LaBSE, BGE-large-zh, Paraphrase-multilingual-mpnet
- Votaci√≥n por consenso para determinar mejores traducciones
- Generaci√≥n de datasets mejorados (top-2 y top-3)

### M√©tricas de Evaluaci√≥n

- **exact_match**: Coincidencia exacta con respuesta esperada (ÊòØ/‰∏çÊòØ) [0-100%]
- **contains_correct**: Respuesta contiene el elemento correcto [0-100%]
- **had_think_tags**: Modelo us√≥ tags de razonamiento interno [0-100%]
- **semantic_similarity**: Similitud sem√°ntica entre traducciones [0-1]

## üìÇ Dataset de Evaluaci√≥n

El dataset consta de 612 preguntas organizadas en 6 bloques estil√≠sticos:

| Bloque | Rango | Estilo | Caracter√≠sticas |
|--------|-------|--------|-----------------|
| 1 | 1-102 | Normal telegr√°fico | Formato est√°ndar, conciso |
| 2 | 103-204 | Formal t√©cnico | Lenguaje culto, ligeramente verborreico |
| 3 | 205-306 | Slang juvenil | Jerga callejera, informal |
| 4 | 307-408 | Con errores | Errores gramaticales no agresivos |
| 5 | 409-510 | Verboso po√©tico | Arcaico, muy elaborado |
| 6 | 511-612 | Minimalista extremo | M√≠nima expresi√≥n posible |

## üöÄ Uso del Sistema

### Instalaci√≥n

```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/catalan-chinese-benchmark.git
cd catalan-chinese-benchmark

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Instalar Ollama (para evaluaci√≥n con LLMs locales)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen3:0.6b
ollama pull yi:9b
ollama pull deepseek-r1:1.5b
```

### Ejecutar Evaluaci√≥n Completa

```
bash
# FASE 1: Traducir dataset con todos los modelos
python t2_nllb200.py datasets/test-catalan.jsonl --output_file test-llm-nllb200.jsonl
python t2_aina.py datasets/test-catalan.jsonl --output_file test-llm-aina.jsonl
python t2_google.py datasets/test-catalan.jsonl --output_file test-llm-gt.jsonl
python t2_m2m100.py datasets/test-catalan.jsonl --output_file test-llm-m2m100.jsonl
python t2_mbart.py datasets/test-catalan.jsonl --output_file test-llm-mbart.jsonl
python t2_opus.py datasets/test-catalan.jsonl --output_file test-llm-opus.jsonl
```

## FASE 2: Evaluar con Ollama
```
bash
python t1_ollama.py
# Configuraci√≥n interactiva:
# - Workers paralelos: 10 (ajustar seg√∫n CPU)
# - Modo TEST: No (para evaluaci√≥n completa)
```

## FASE 3: Crear ensemble sem√°ntico
```
bash
python t3_ensemble.py --data_dir . --output_dir ./ranked_datasets --top_k 2 3
```
### Evaluaci√≥n R√°pida (Modo Test)

```
bash
# Solo 10 preguntas para prueba r√°pida
python t1_ollama.py --test_mode --num_test 10
```

### üìä Estructura del Proyecto

```
catalan-chinese-benchmark/
‚îú‚îÄ‚îÄ README.md                          # Este archivo
‚îú‚îÄ‚îÄ requirements.txt                   # Dependencias Python
‚îú‚îÄ‚îÄ requirements_tpu.txt              # Dependencias para TPU/Colab
‚îÇ
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ test-catalan.jsonl           # Dataset original (612 preguntas)
‚îÇ   ‚îî‚îÄ‚îÄ explain_dataset.txt          # Descripci√≥n de bloques
‚îÇ
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ t1_ollama.py                 # Evaluaci√≥n con LLMs locales
‚îÇ   ‚îú‚îÄ‚îÄ t2_nllb200.py                # Traducci√≥n con NLLB-200
‚îÇ   ‚îú‚îÄ‚îÄ t2_aina.py                   # Traducci√≥n con AINA
‚îÇ   ‚îú‚îÄ‚îÄ t2_google.py                 # Traducci√≥n con Google
‚îÇ   ‚îú‚îÄ‚îÄ t2_m2m100.py                 # Traducci√≥n con M2M100
‚îÇ   ‚îú‚îÄ‚îÄ t2_mbart.py                  # Traducci√≥n con mBART
‚îÇ   ‚îú‚îÄ‚îÄ t2_opus.py                   # Traducci√≥n con OpusMT
‚îÇ   ‚îî‚îÄ‚îÄ t3_ensemble.py               # Ensemble sem√°ntico
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ summary_results_*.json       # Resultados por modelo
‚îÇ   ‚îú‚îÄ‚îÄ ranking_statistics.json      # Estad√≠sticas de ensemble
‚îÇ   ‚îî‚îÄ‚îÄ results_direct/              # Resultados detallados
‚îÇ
‚îú‚îÄ‚îÄ training/                         # Sistema de entrenamiento (opcional)
‚îÇ   ‚îú‚îÄ‚îÄ train_all.py                 # Entrenamiento multi-modelo
‚îÇ   ‚îú‚îÄ‚îÄ universal_trainer.py         # L√≥gica de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ model_configs.py             # Configuraciones de modelos
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_manager.py        # Gesti√≥n de checkpoints
‚îÇ
‚îî‚îÄ‚îÄ ranked_datasets/                  # Datasets generados por ensemble
    ‚îú‚îÄ‚îÄ top2_ranked_dataset.jsonl    # Mejores 2 traducciones
    ‚îî‚îÄ‚îÄ top3_ranked_dataset.jsonl    # Mejores 3 traducciones
```

### üîßConfiguraci√≥n Avanzada
Ajustar Par√°metros de Evaluaci√≥n
```
`python
# En t1_ollama.py
config.iterations_per_question = 10  # M√°s iteraciones para mayor robustez
config.max_parallel_calls = 20       # M√°s workers para evaluaci√≥n r√°pida
config.timeout_seconds = 120         # Timeout mayor para modelos lentos
```

### Agregar Nuevo Modelo de Traducci√≥n

Crear script t2_nuevo_modelo.py basado en plantilla existente
Implementar clase NuevoModeloTranslator
Agregar a pipeline de evaluaci√≥n

### Agregar Nuevo LLM Evaluador
```
python
# En t1_ollama.py, agregar a config.llm_models
self.llm_models = [
    "qwen3:0.6b",
    "yi:9b",
    "deepseek-r1:1.5b",
    "nuevo-modelo:version"  # Nuevo LLM
]
```
## üìùReproducibilidad

Para reproducir exactamente los resultados:

Versiones de software: Ver requirements.txt para versiones exactas
Semillas aleatorias: Todas fijadas a 42
Datasets: Disponibles en carpeta datasets/
Configuraci√≥n: Documentada en archivos de configuraci√≥n

## üìà Resultados Principales

### üî¨ Experimento de Control: Llama3.2:3b

### Contexto del Experimento
Llama3.2:3b se utiliz√≥ como modelo de control para evaluar el comportamiento de un LLM que **no conoce ni catal√°n ni chino** cuando se le presentan directamente preguntas S√≠/No en catal√°n y se espera respuesta en formato chino (ÊòØ/‰∏çÊòØ).

#### M√©tricas Principales de Control

| M√©trica | Valor | Interpretaci√≥n |
|---------|-------|----------------|
| **Exact Match** | 0.0% | No puede generar formato chino |
| **Contains Correct** | 97.3% | Comprende perfectamente el contenido |
| **Idioma de Respuesta** | 94.6% Espa√±ol, 2.1% Catal√°n | Responde en idiomas conocidos |
| **Respuestas Vac√≠as** | 84.6% | Alta tasa de no-respuesta al formato esperado |
| **Consistencia** | 85.5% | Muy consistente en sus respuestas |

#### An√°lisis por Bloques Estil√≠sticos

| Bloque | Contains Correct | Respuesta Media | Consistencia |
|--------|-----------------|-----------------|--------------|
| **Normal Telegr√°fico** | 100% | 3.3 chars | 98.8% |
| **Formal T√©cnico** | 100% | 3.0 chars | 98.5% |
| **Slang Juvenil** | 100% | 3.6 chars | 97.8% |
| **Errores Gramaticales** | 99.4% | 32.2 chars | 87.0% |
| **Verboso Po√©tico** | 100% | 3.0 chars | 99.0% |
| **Minimalista Extremo** | 84.5% | 333.5 chars | 31.9% |

#### Patrones de Respuesta Observados

#### Distribuci√≥n de Respuestas
- **"S√≠" en espa√±ol**: 973 casos (31.8%)
- **"No" en espa√±ol**: 1,631 casos (53.3%)
- **Respuestas elaboradas**: 16.6% (con explicaci√≥n)
- **Sin respuesta clara**: 80.8%

#### Top Palabras en Respuestas
1. "No." - 1,631 ocurrencias
2. "S√≠." - 973 ocurrencias
3. Palabras en espa√±ol: "de", "la", "que", "el", "es"
4. Sin t√©rminos en chino en las respuestas

#### Hallazgos Clave del Control

1. **Comprensi√≥n Perfecta**: 97.3% de comprensi√≥n del contenido demuestra que el modelo entiende las preguntas en catal√°n a pesar de no estar entrenado espec√≠ficamente en este idioma

2. **Incapacidad de Formato**: 0% exact match confirma que sin conocimiento del chino, no puede generar el formato de respuesta esperado (ÊòØ/‰∏çÊòØ)

3. **Respuesta en Idioma Conocido**: El modelo defaultea a espa√±ol (94.6%) como idioma de respuesta, su idioma m√°s cercano al catal√°n

4. **Alta Consistencia**: 85.5% de consistencia indica que el modelo es muy predecible en su comportamiento, especialmente en bloques simples

5. **Degradaci√≥n con Complejidad**: El bloque minimalista extremo muestra una ca√≠da dram√°tica en consistencia (31.9%) y comprensi√≥n (84.5%)

#### Comparaci√≥n con Modelos de Traducci√≥n

| Aspecto | Llama3.2:3b (Control) | Mejor Traductor (AINA) |
|---------|----------------------|------------------------|
| **Comprensi√≥n Sem√°ntica** | 97.3% | 69.0% |
| **Formato Correcto** | 0% | 47.0% |
| **Consistencia** | 85.5% | 74.4% |
| **Respuestas V√°lidas** | 15.4% (intentos) | 98.8% (intentos) |

#### Implicaciones del Control

1. **Validaci√≥n del Pipeline**: Confirma que la traducci√≥n es esencial para obtener respuestas en formato chino

2. **Benchmark de Comprensi√≥n**: Establece un techo de 97.3% para la comprensi√≥n sem√°ntica pura de las preguntas

3. **Importancia del Conocimiento Ling√º√≠stico**: Demuestra que sin conocimiento espec√≠fico del chino, incluso con comprensi√≥n perfecta, es imposible generar el formato de respuesta correcto

4. **Robustez Cross-ling√º√≠stica**: Sugiere que los LLMs pueden comprender idiomas relacionados (catal√°n) a trav√©s de idiomas conocidos (espa√±ol)

5. **Limitaciones del Minimalismo**: El estilo minimalista extremo es desafiante incluso para la comprensi√≥n, no solo para la traducci√≥n

### Ranking Global de Modelos de Traducci√≥n

| Ranking | Modelo | Exact Match | Contains Correct |
|---------|--------|-------------|------------------|
| 1¬∫ | **AINA** | 46.98% | 68.97% |
| 2¬∫ | **Google Translate** | 41.97% | 63.28% |
| 3¬∫ | **Ensemble (Top-3)** | 25.84% | 62.35% |
| 4¬∫ | **mBART** | 19.55% | 54.28% |
| 5¬∫ | **M2M100** | 16.60% | 53.88% |
| 6¬∫ | **CAT (Directo)¬π** | 2.70% | 89.85% |

¬π *CAT: Evaluaci√≥n directa en catal√°n sin traducci√≥n - Alta comprensi√≥n sem√°ntica (89.85%) pero falla en formato de respuesta chino*  
¬≤ *Llama3.2:3b: Modelo base de trabajo del proyecto*

#### Rendimiento por Estilo Ling√º√≠stico (Exact Match % - Promedio entre evaluadores)

| Modelo | Formal T√©cnico | Errores Gramaticales | Minimalista Extremo | Normal Telegr√°fico | Slang Juvenil | Verboso Po√©tico |
|--------|----------------|---------------------|-------------------|------------------|---------------|-----------------|
| **AINA** | 56.5% | 45.1% | 28.1% | 53.0% | 50.9% | 41.1% |
| **Google** | 50.1% | 46.1% | 18.3% | 37.6% | 43.6% | 34.4% |
| **Ensemble** | 31.0% | 21.2% | 16.1% | 31.6% | 24.8% | 30.4% |
| **mBART** | 27.7% | 20.7% | 13.7% | 18.4% | 20.2% | 20.2% |
| **M2M100** | 23.8% | 7.3% | 19.7% | 15.5% | 12.2% | 21.2% |
| **CAT** | 3.3% | 0.3% | 1.1% | 4.6% | 2.3% | 4.2% |

#### M√©tricas Detalladas por Modelo Evaluador

#### Modelo: AINA
| LLM Evaluador | Exact Match | Contains Correct |
|---------------|-------------|------------------|
| Qwen3:0.6b | 55.64% | 70.86% |
| Yi:9b | 36.10% | 72.60% |
| DeepSeek-R1:1.5b | 49.06% | 63.47% |

#### Modelo: Google Translate  
| LLM Evaluador | Exact Match | Contains Correct |
|---------------|-------------|------------------|
| Qwen3:0.6b | 57.98% | 65.05% |
| Yi:9b | 26.32% | 66.71% |
| DeepSeek-R1:1.5b | 41.57% | 58.03% |

#### Modelo: CAT (Evaluaci√≥n Directa en Catal√°n)
| LLM Evaluador | Exact Match | Contains Correct |
|---------------|-------------|------------------|
| Qwen3:0.6b | 7.85% | 88.25% |
| Yi:9b | 0.00% | 95.01% |
| DeepSeek-R1:1.5b | 0.03% | 86.59% |

### üìä An√°lisis Detallado de Resultados

#### üîç An√°lisis de Patrones de Respuesta

| Modelo | Sesgo de Respuesta | Direcci√≥n | Respuestas Mixtas | Respuestas Vac√≠as |
|--------|-------------------|-----------|-------------------|-------------------|
| **AINA** | 24.7% | Hacia "S√≠" | 10.2% | 1.2% |
| **Google Translate** | 17.3% | Hacia "S√≠" | 11.6% | 1.1% |
| **M2M100** | 42.7% | Hacia "S√≠" | 49.9% | 2.1% |
| **mBART** | 34.5% | Hacia "S√≠" | 36.9% | 1.6% |
| **CAT (Directo)** | 15.1% | Hacia "No" | 24.7% | 46.4% |
| **Ensemble Top-3** | 41.0% | Hacia "S√≠" | 27.0% | 0.7% |

#### üìè An√°lisis de Longitud y Verbosidad

| Modelo | Longitud Media | Mediana | Longitud √ìptima | Correlaci√≥n Longitud-Precisi√≥n |
|--------|---------------|---------|-----------------|--------------------------------|
| **AINA** | 19.3 chars | 1 | 1 (73.1% precisi√≥n) | -0.271 |
| **Google Translate** | 26.7 chars | 1 | 1 (69.1% precisi√≥n) | -0.275 |
| **M2M100** | 141.8 chars | 102 | 1 (85.6% precisi√≥n) | -0.408 |
| **mBART** | 116.4 chars | 38 | 1 (78.6% precisi√≥n) | -0.341 |
| **CAT (Directo)** | 185.2 chars | 98 | 2 (46.7% precisi√≥n) | -0.133 |
| **Ensemble Top-3** | 57.3 chars | 3 | 1 (72.0% precisi√≥n) | -0.268 |

#### üéØ An√°lisis de Consistencia

| Modelo | Score de Consistencia | Preguntas Determin√≠sticas | Preguntas Ca√≥ticas | Alta Confianza |
|--------|----------------------|--------------------------|-------------------|----------------|
| **AINA** | 74.4% ¬± 18.5% | 26 | 2 | 595 |
| **Google Translate** | 71.8% ¬± 22.2% | 5 | 16 | 575 |
| **M2M100** | 22.6% ¬± 25.1% | 3 | 220 | 276 |
| **mBART** | 38.5% ¬± 29.2% | 4 | 123 | 394 |
| **CAT (Directo)** | 33.5% ¬± 21.0% | 0 | 73 | 385 |
| **Ensemble Top-3** | 53.5% ¬± 25.7% | 1 | 17 | 487 |

### üìà An√°lisis Temporal y Fatiga

#### Degradaci√≥n de Rendimiento (Q1 ‚Üí Q4)

| Modelo | Q1 Accuracy | Q4 Accuracy | Tendencia | Degradaci√≥n |
|--------|------------|------------|-----------|-------------|
| **AINA** | 54.0% | 37.8% | ‚Üì Degradaci√≥n | -16.2% |
| **Google Translate** | 49.7% | 31.0% | ‚Üì Degradaci√≥n | -18.7% |
| **M2M100** | 18.5% | 21.4% | ‚Üë Mejora | +2.9% |
| **mBART** | 20.6% | 16.3% | ‚Üì Degradaci√≥n | -4.3% |
| **CAT (Directo)** | 4.0% | 2.6% | ‚Üì Degradaci√≥n | -1.4% |
| **Ensemble Top-3** | 29.8% | 21.4% | ‚Üì Degradaci√≥n | -8.4% |

### üí™ M√©tricas de Robustez

#### Sensibilidad al Estilo y Adaptabilidad

| Modelo | Sensibilidad (œÉ) | Rango de Variaci√≥n | Resiliencia a Errores | Tendencia de Complejidad |
|--------|-----------------|-------------------|----------------------|-------------------------|
| **AINA** | 0.074 | 21.8% | -7.9% drop | ‚Üì -0.037 (degrada) |
| **Google Translate** | 0.080 | 22.7% | -4.9% drop | ‚Üì -0.045 (degrada) |
| **M2M100** | 0.057 | 16.5% | -8.3% drop | ‚Üë +0.005 (mejora) |
| **mBART** | 0.039 | 13.0% | +0.5% mejora | ‚Üì -0.011 (degrada) |
| **CAT (Directo)** | 0.016 | 4.3% | -4.3% drop | ‚Üì -0.003 (degrada) |
| **Ensemble Top-3** | 0.057 | 15.4% | -10.1% drop | ‚Üì -0.022 (degrada) |

### üî¨ Casos Especiales

#### An√°lisis de Falsos Positivos y Casos Extremos

| Modelo | Falsos Positivos | Respuestas Extremas (>P95) | Preguntas Perfectas | Preguntas Imposibles |
|--------|-----------------|---------------------------|-------------------|---------------------|
| **AINA** | 22.0% (1,988) | 1,498 casos (max: 947) | 25 (4.1%) | 37 (6.0%) |
| **Google Translate** | 21.3% (1,939) | 1,742 casos (max: 1,113) | 5 (0.8%) | 38 (6.2%) |
| **M2M100** | 37.3% (3,370) | 381 casos (max: 2,384) | 3 (0.5%) | 308 (50.3%) |
| **mBART** | 34.7% (3,125) | 708 casos (max: 1,639) | 4 (0.7%) | 249 (40.7%) |
| **CAT (Directo)** | 89.0% (7,888) | 179 casos (max: 1,821) | 0 (0%) | 506 (82.7%) |
| **Ensemble Top-3** | 36.5% (3,223) | 1,743 casos (max: 1,907) | 1 (0.2%) | 118 (19.3%) |

### üìâ Rendimiento por Complejidad Estil√≠stica

#### Degradaci√≥n Promedio por Bloque (ordenado por dificultad)

| Bloque | AINA | Google | M2M100 | mBART | CAT | Ensemble |
|--------|------|--------|--------|-------|-----|----------|
| **Normal Telegr√°fico** | 53.0% | 51.0% | 15.5% | 18.0% | 4.6% | 31.3% |
| **Formal T√©cnico** | 56.5% | 48.4% | 23.8% | 26.6% | 3.3% | 31.1% |
| **Errores Gramaticales** | 45.1% | 46.0% | 7.2% | 18.6% | 0.3% | 21.2% |
| **Slang Juvenil** | 50.9% | 43.6% | 12.2% | 20.2% | 2.3% | 24.7% |
| **Verboso Po√©tico** | 41.3% | 34.3% | 21.3% | 20.3% | 4.4% | 30.1% |
| **Minimalista Extremo** | 34.7% | 28.3% | 19.7% | 13.6% | 1.2% | 15.9% |

### üéØ M√©tricas de Calidad Agregadas

| Modelo | F1-Score Promedio | Precisi√≥n Ponderada | Tasa Comprensi√≥n Real | √çndice de Formato |
|--------|------------------|--------------------|--------------------|-------------------|
| **AINA** | 46.9% | 49.2% | 22.0% | 47.6% √©xito cuando intenta |
| **Google Translate** | 41.9% | 44.5% | 21.3% | 42.4% √©xito cuando intenta |
| **M2M100** | 16.6% | 16.7% | 37.3% | 17.0% √©xito cuando intenta |
| **mBART** | 19.5% | 20.1% | 34.7% | 19.9% √©xito cuando intenta |
| **CAT (Directo)** | 2.7% | 3.0% | 87.1% | 5.0% √©xito cuando intenta |
| **Ensemble Top-3** | 25.7% | 27.2% | 36.5% | 26.0% √©xito cuando intenta |

### üìä Conclusiones del An√°lisis Detallado

1. **Modelo m√°s consistente**: Llama3.2:3b (85.5%) pero con 0% de precisi√≥n
2. **Modelo m√°s preciso**: AINA (46.9% F1-Score) con buena consistencia (74.4%)
3. **Mejor balance precisi√≥n-consistencia**: AINA y Google Translate
4. **Mayor degradaci√≥n temporal**: Google Translate (-18.7%) y AINA (-16.2%)
5. **M√°s sensible al estilo**: Google Translate (œÉ=0.080) y AINA (œÉ=0.074)
6. **Bloque m√°s desafiante**: Minimalista extremo (promedio 15.5% entre todos los modelos)
7. **Bloque m√°s f√°cil**: Formal t√©cnico (promedio 28.3% entre todos los modelos)

### Hallazgos Clave

1. **AINA lidera en precisi√≥n global** con el mejor balance entre exact match (46.98%) y comprensi√≥n sem√°ntica (68.97%), estableci√©ndose como el modelo de traducci√≥n m√°s efectivo para el par catal√°n‚Üíchino

2. **Experimento de control (CAT)**: La evaluaci√≥n directa en catal√°n demuestra que los LLMs comprenden excelentemente el contenido (89.85% contains_correct) pero no pueden generar respuestas en el formato chino esperado (ÊòØ/‰∏çÊòØ), validando la necesidad de traducci√≥n

3. **Modelo Llama3.2:3b**: Aunque muestra comprensi√≥n casi perfecta del contenido (97.32%), su incapacidad para generar el formato de respuesta correcto lo hace inadecuado como traductor directo, justificando para √©l tambi√©n el pipeline de traducci√≥n+evaluaci√≥n

4. **Impacto del estilo ling√º√≠stico**: 
   - **Formal t√©cnico**: Mejores resultados generales (AINA: 56.5%, Google: 50.1%)
   - **Minimalista extremo**: El m√°s desafiante para todos los modelos (AINA: 28.1%, Google: 18.3%)
   - **Errores gramaticales**: Impacto moderado en la precisi√≥n, sugiriendo robustez de los modelos

5. **Efecto del razonamiento interno**: 
   - Modelos con think tags (Qwen3, DeepSeek-R1): Mayor precisi√≥n en exact match
   - Yi:9b sin think tags: Mayor contains_correct pero menor precisi√≥n exacta
   - Correlaci√≥n positiva entre uso de razonamiento interno y precisi√≥n de formato

6. **Ensemble sem√°ntico**: Con 25.84% de exact match, el ensemble no supera a los mejores modelos individuales pero ofrece valor para aplicaciones que requieren diversidad y consenso entre traducciones

### Distribuci√≥n de Evaluaciones

- **Total de evaluaciones**: 54,172 (promedio 9,029 por modelo)
- **Preguntas evaluadas**: 612 en 6 estilos ling√º√≠sticos distintos
- **Iteraciones por pregunta**: 5 para robustez estad√≠stica
- **LLMs evaluadores**: 3 modelos (Qwen3:0.6b, Yi:9b, DeepSeek-R1:1.5b)
- **Modelos de traducci√≥n evaluados**: 6 + 2 controles (CAT directo, Llama3.2:3b base)

### Contribuciones del Ensemble (Top-2)

AINA: 340 contribuciones (27.8%)
NLLB: 321 contribuciones (26.2%)
M2M100: 257 contribuciones (21.0%)
Google: 195 contribuciones (15.9%)
mBART: 111 contribuciones (9.1%)

## ü§ùContribuciones

Las contribuciones son bienvenidas. Por favor:

Fork el repositorio
Crea una rama para tu feature (git checkout -b feature/NuevaCaracteristica)
Commit tus cambios (git commit -m 'Agregar nueva caracter√≠stica')
Push a la rama (git push origin feature/NuevaCaracteristica)
Abre un Pull Request

## üìÑLicencia
Este proyecto est√° bajo licencia MIT. Ver archivo LICENSE para m√°s detalles.

## üôèAgradecimientos

Modelos de traducci√≥n de c√≥digo abierto
Comunidad Ollama por infraestructura de LLMs locales
Sentence Transformers por modelos de embeddings

