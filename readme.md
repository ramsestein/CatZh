# ğŸŒ Benchmark de TraducciÃ³n CatalÃ¡n-Chino mediante EvaluaciÃ³n LLM

## ğŸ“Š Resumen del Proyecto

Sistema innovador de evaluaciÃ³n de calidad de traducciÃ³n automÃ¡tica catalÃ¡nâ†’chino que utiliza la capacidad de LLMs para responder preguntas SÃ­/No como mÃ©trica de calidad. Este benchmark evalÃºa 6 modelos de traducciÃ³n mediante 3 LLMs evaluadores sobre un dataset de 612 preguntas en 6 estilos lingÃ¼Ã­sticos diferentes.

### ğŸ¯ CaracterÃ­sticas Principales

- **612 preguntas SÃ­/No** en catalÃ¡n con respuestas esperadas en chino (æ˜¯/ä¸æ˜¯)
- **6 modelos de traducciÃ³n** evaluados: NLLB-200, AINA, Google Translate, M2M100, mBART, OpusMT
- **3 LLMs evaluadores**: Qwen3:0.6b, Yi:9b, DeepSeek-R1:1.5b
- **6 estilos lingÃ¼Ã­sticos**: Desde formal tÃ©cnico hasta minimalista extremo
- **Sistema de ensemble** con ranking semÃ¡ntico usando 3 modelos de embeddings
- **9,040+ evaluaciones** por modelo (5 iteraciones Ã— 612 preguntas Ã— 3 LLMs)

## ğŸ”¬ MetodologÃ­a

### Pipeline de EvaluaciÃ³n de 3 Fases

**FASE 1: TRADUCCIÃ“N**
- Dataset Original: 612 preguntas en catalÃ¡n
- 6 Modelos de TraducciÃ³n: NLLB-200, AINA, Google Translate, M2M100, mBART, OpusMT
- Resultado: 6 datasets traducidos al chino

**FASE 2: EVALUACIÃ“N CON LLMs**
- 3 LLMs evalÃºan cada dataset traducido
- 5 iteraciones por pregunta para robustez estadÃ­stica
- MÃ©tricas: exact_match, contains_correct, had_think_tags
- Total: 9,040 evaluaciones por modelo

**FASE 3: ENSEMBLE SEMÃNTICO**
- Ranking usando 3 modelos de embeddings: LaBSE, BGE-large-zh, Paraphrase-multilingual-mpnet
- VotaciÃ³n por consenso para determinar mejores traducciones
- GeneraciÃ³n de datasets mejorados (top-2 y top-3)

### MÃ©tricas de EvaluaciÃ³n

- **exact_match**: Coincidencia exacta con respuesta esperada (æ˜¯/ä¸æ˜¯) [0-100%]
- **contains_correct**: Respuesta contiene el elemento correcto [0-100%]
- **had_think_tags**: Modelo usÃ³ tags de razonamiento interno [0-100%]
- **semantic_similarity**: Similitud semÃ¡ntica entre traducciones [0-1]

## ğŸ“ˆ Resultados Principales

### Ranking Global de Modelos de TraducciÃ³n

| Ranking | Modelo | Exact Match | Contains Correct | Score Ensemble |
|---------|--------|-------------|------------------|----------------|
| 1Âº | NLLB-200 | 46.98% | 68.97% | 4.40 pts |
| 2Âº | AINA | 46.98% | 68.97% | 4.58 pts |
| 3Âº | Google Translate | 41.97% | 63.28% | 3.82 pts |
| 4Âº | M2M100 | 16.60% | 53.88% | 4.10 pts |
| 5Âº | mBART | 19.55% | 54.28% | 2.97 pts |
| 6Âº | OpusMT | 0.00% | 97.32% | 1.14 pts |

### Rendimiento por Estilo LingÃ¼Ã­stico (Exact Match %)

| Modelo | Normal TelegrÃ¡fico | Formal TÃ©cnico | Slang Juvenil | Con Errores | Verboso PoÃ©tico | Minimalista |
|--------|-------------------|----------------|---------------|-------------|-----------------|-------------|
| NLLB | 52.4% | 60.4% | 64.6% | 51.8% | 46.5% | 42.1% |
| AINA | 52.4% | 60.8% | 53.3% | 47.6% | 56.1% | 23.8% |
| Google | 71.9% | 70.6% | 60.0% | 59.5% | 55.6% | 30.0% |
| M2M100 | 6.0% | 16.9% | 5.1% | 3.6% | 16.1% | 18.4% |
| mBART | 11.8% | 21.0% | 14.3% | 15.4% | 21.5% | 12.3% |
| OpusMT | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |

## ğŸ“‚ Dataset de EvaluaciÃ³n

El dataset consta de 612 preguntas organizadas en 6 bloques estilÃ­sticos:

| Bloque | Rango | Estilo | CaracterÃ­sticas |
|--------|-------|--------|-----------------|
| 1 | 1-102 | Normal telegrÃ¡fico | Formato estÃ¡ndar, conciso |
| 2 | 103-204 | Formal tÃ©cnico | Lenguaje culto, ligeramente verborreico |
| 3 | 205-306 | Slang juvenil | Jerga callejera, informal |
| 4 | 307-408 | Con errores | Errores gramaticales no agresivos |
| 5 | 409-510 | Verboso poÃ©tico | Arcaico, muy elaborado |
| 6 | 511-612 | Minimalista extremo | MÃ­nima expresiÃ³n posible |

## ğŸš€ Uso del Sistema

### InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/catalan-chinese-benchmark.git
cd catalan-chinese-benchmark

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Instalar Ollama (para evaluaciÃ³n con LLMs locales)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen3:0.6b
ollama pull yi:9b
ollama pull deepseek-r1:1.5b

###Ejecutar EvaluaciÃ³n Completa

```
bash
# FASE 1: Traducir dataset con todos los modelos
python t2_nllb200.py datasets/test-catalan.jsonl --output_file test-llm-nllb200.jsonl
python t2_aina.py datasets/test-catalan.jsonl --output_file test-llm-aina.jsonl
python t2_google.py datasets/test-catalan.jsonl --output_file test-llm-gt.jsonl
python t2_m2m100.py datasets/test-catalan.jsonl --output_file test-llm-m2m100.jsonl
python t2_mbart.py datasets/test-catalan.jsonl --output_file test-llm-mbart.jsonl
python t2_opus.py datasets/test-catalan.jsonl --output_file test-llm-opus.jsonl

# FASE 2: Evaluar con Ollama
python t1_ollama.py
# ConfiguraciÃ³n interactiva:
# - Workers paralelos: 10 (ajustar segÃºn CPU)
# - Modo TEST: No (para evaluaciÃ³n completa)

# FASE 3: Crear ensemble semÃ¡ntico
python t3_ensemble.py --data_dir . --output_dir ./ranked_datasets --top_k 2 3
```
###EvaluaciÃ³n RÃ¡pida (Modo Test)

```
bash
# Solo 10 preguntas para prueba rÃ¡pida
python t1_ollama.py --test_mode --num_test 10
```

###ğŸ“Š Estructura del Proyecto

catalan-chinese-benchmark/
â”œâ”€â”€ README.md                          # Este archivo
â”œâ”€â”€ requirements.txt                   # Dependencias Python
â”œâ”€â”€ requirements_tpu.txt              # Dependencias para TPU/Colab
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ test-catalan.jsonl           # Dataset original (612 preguntas)
â”‚   â””â”€â”€ explain_dataset.txt          # DescripciÃ³n de bloques
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ t1_ollama.py                 # EvaluaciÃ³n con LLMs locales
â”‚   â”œâ”€â”€ t2_nllb200.py                # TraducciÃ³n con NLLB-200
â”‚   â”œâ”€â”€ t2_aina.py                   # TraducciÃ³n con AINA
â”‚   â”œâ”€â”€ t2_google.py                 # TraducciÃ³n con Google
â”‚   â”œâ”€â”€ t2_m2m100.py                 # TraducciÃ³n con M2M100
â”‚   â”œâ”€â”€ t2_mbart.py                  # TraducciÃ³n con mBART
â”‚   â”œâ”€â”€ t2_opus.py                   # TraducciÃ³n con OpusMT
â”‚   â””â”€â”€ t3_ensemble.py               # Ensemble semÃ¡ntico
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ summary_results_*.json       # Resultados por modelo
â”‚   â”œâ”€â”€ ranking_statistics.json      # EstadÃ­sticas de ensemble
â”‚   â””â”€â”€ results_direct/              # Resultados detallados
â”‚
â”œâ”€â”€ training/                         # Sistema de entrenamiento (opcional)
â”‚   â”œâ”€â”€ train_all.py                 # Entrenamiento multi-modelo
â”‚   â”œâ”€â”€ universal_trainer.py         # LÃ³gica de entrenamiento
â”‚   â”œâ”€â”€ model_configs.py             # Configuraciones de modelos
â”‚   â””â”€â”€ checkpoint_manager.py        # GestiÃ³n de checkpoints
â”‚
â””â”€â”€ ranked_datasets/                  # Datasets generados por ensemble
    â”œâ”€â”€ top2_ranked_dataset.jsonl    # Mejores 2 traducciones
    â””â”€â”€ top3_ranked_dataset.jsonl    # Mejores 3 traducciones


###ğŸ“‰ AnÃ¡lisis de Resultados
##Hallazgos Clave

- OpusMT: Paradoja interesante - 97.3% contains_correct pero 0% exact_match, sugiere que entiende el contenido pero no respeta el formato de respuesta
- NLLB y AINA: Mejor balance entre comprensiÃ³n y formato, lideran en el ranking del ensemble
- Google Translate: Rendimiento sÃ³lido y consistente en todos los estilos lingÃ¼Ã­sticos

Impacto del estilo: El bloque minimalista extremo es el mÃ¡s desafiante para todos los modelos
Think tags: Los modelos que usan razonamiento interno (Qwen3, DeepSeek) muestran mayor precisiÃ³n

##Contribuciones del Ensemble (Top-2)

AINA: 340 contribuciones (27.8%)
NLLB: 321 contribuciones (26.2%)
M2M100: 257 contribuciones (21.0%)
Google: 195 contribuciones (15.9%)
mBART: 111 contribuciones (9.1%)

###ğŸ”§ ConfiguraciÃ³n Avanzada
Ajustar ParÃ¡metros de EvaluaciÃ³n
```
`python
# En t1_ollama.py
config.iterations_per_question = 10  # MÃ¡s iteraciones para mayor robustez
config.max_parallel_calls = 20       # MÃ¡s workers para evaluaciÃ³n rÃ¡pida
config.timeout_seconds = 120         # Timeout mayor para modelos lentos
```

##Agregar Nuevo Modelo de TraducciÃ³n

Crear script t2_nuevo_modelo.py basado en plantilla existente
Implementar clase NuevoModeloTranslator
Agregar a pipeline de evaluaciÃ³n

##Agregar Nuevo LLM Evaluador
```
python
# En t1_ollama.py, agregar a config.llm_models
self.llm_models = [
    "qwen3:0.6b",
    "yi:9b",
    "deepseek-r1:1.5b",
    "nuevo-modelo:version"  # Nuevo LLM
]
```

###ğŸ“ Reproducibilidad

Para reproducir exactamente los resultados:

Versiones de software: Ver requirements.txt para versiones exactas
Semillas aleatorias: Todas fijadas a 42
Datasets: Disponibles en carpeta datasets/
ConfiguraciÃ³n: Documentada en archivos de configuraciÃ³n

###ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

Fork el repositorio
Crea una rama para tu feature (git checkout -b feature/NuevaCaracteristica)
Commit tus cambios (git commit -m 'Agregar nueva caracterÃ­stica')
Push a la rama (git push origin feature/NuevaCaracteristica)
Abre un Pull Request

###ğŸ“„ Licencia
Este proyecto estÃ¡ bajo licencia MIT. Ver archivo LICENSE para mÃ¡s detalles.

###ğŸ“§ Contacto
Para preguntas o colaboraciones, contactar a: [tu-email@ejemplo.com]

###ğŸ™ Agradecimientos

Modelos de traducciÃ³n de cÃ³digo abierto
Comunidad Ollama por infraestructura de LLMs locales
Sentence Transformers por modelos de embeddings

